{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191b142-9c38-45a6-9de8-ac78e1a10385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./2023-02-26_SegLenTestsOnMlcNetC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab5d19-0706-4c0a-b29d-1dd3301c6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [\"5s\", \"10s\", \"20s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64f7ab-c520-4b7d-be99-fb40efa2b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_std_plot_params(fig):\n",
    "    # Set aesthetics.\n",
    "    fig.update_layout(font = dict(size = 24))\n",
    "    # Remove the background coloring.\n",
    "    fig.update_layout({\"plot_bgcolor\": \"rgba(0,0,0,0)\",\n",
    "                       \"paper_bgcolor\": \"rgba(0,0,0,0)\"})\n",
    "    # Make the gridlines visible on the transparent background.\n",
    "    fig.update_xaxes(showgrid = True, gridwidth = 1, gridcolor = \"rgba(169,169,169,0.5)\")\n",
    "    fig.update_yaxes(showgrid = True, gridwidth = 1, gridcolor = \"rgba(169,169,169,0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa61ad8-4b00-4850-a253-f64466b800bf",
   "metadata": {},
   "source": [
    "### Get length of video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b62ac-4362-4ddf-b9b5-0c7e00222c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first JS log file in the directory and find out how long the video is. I assume that all log data in this entire directory has the same length of video.\n",
    "import os\n",
    "import re\n",
    "SEC_IN_MIN = 60\n",
    "seg_lens = [\"5s\", \"10s\", \"20s\"]\n",
    "vid_length_pattern = re.compile(\"^LOG  [\\d\\.]+,([\\d\\.]+):.+$\", re.M)\n",
    "\n",
    "video_length_sec = None\n",
    "\n",
    "for seg_len in seg_lens:\n",
    "    dir_path = \"./{seg_len}\".format(seg_len = seg_len)\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        jsLog_names = [filename for filename in os.listdir(file_path) if filename.startswith(\"Tester_\") and filename.endswith(\".log\")]\n",
    "        if len(jsLog_names) != 1:\n",
    "            continue\n",
    "        jsLog_name = jsLog_names[0]\n",
    "        \n",
    "        last_line = None\n",
    "        with open(os.path.join(file_path, jsLog_name), \"r\") as jsLog:\n",
    "            for line in jsLog:\n",
    "                pass\n",
    "            last_line = line\n",
    "        \n",
    "        match = vid_length_pattern.match(last_line)\n",
    "        if not match:\n",
    "            continue\n",
    "        video_length_sec = match.group(1)\n",
    "        break\n",
    "        \n",
    "    if video_length_sec is not None:\n",
    "        break\n",
    "\n",
    "video_length_sec = float(video_length_sec)\n",
    "print(\"Video length (sec): {vidlen}\".format(vidlen = video_length_sec))\n",
    "video_length_min = video_length_sec / SEC_IN_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e9bb0-b064-4af1-905b-59791b05b3ed",
   "metadata": {},
   "source": [
    "## Stalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a925e03-f68e-481d-a87a-da641dcfe58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "stall_start_pattern = re.compile(\"^STALL  started at ([\\d\\.]+) sec\", re.M)\n",
    "stall_end_pattern = re.compile(\"^STALL  ([\\d]+) ms and stopped at [\\d\\.]+ sec\", re.M)\n",
    "\n",
    "def ComputeStallData(dir_path):\n",
    "    jsLog_names = [filename for filename in os.listdir(dir_path) if filename.startswith(\"Tester_\") and filename.endswith(\".log\")]\n",
    "    if len(jsLog_names) != 1:\n",
    "        return\n",
    "    jsLog_name = jsLog_names[0]\n",
    "    jsLog = open(os.path.join(dir_path, jsLog_name), \"r\")\n",
    "    \n",
    "    stall_starts = list()\n",
    "    stall_lengths_ms = list()\n",
    "    \n",
    "    # Read through all the lines.\n",
    "    while line := jsLog.readline():\n",
    "        start_match = stall_start_pattern.match(line)\n",
    "        if start_match:\n",
    "            stall_start_sec = float(start_match.group(1))\n",
    "            stall_starts.append(stall_start_sec)\n",
    "        \n",
    "        end_match = stall_end_pattern.match(line)\n",
    "        if end_match:\n",
    "            stall_len_ms = int(end_match.group(1))\n",
    "            stall_lengths_ms.append(stall_len_ms)\n",
    "    \n",
    "    return stall_starts, stall_lengths_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0076fe-d4b0-481d-a505-1b0b58d9f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats as st\n",
    "\n",
    "seg_lens = [\"5s\", \"10s\", \"20s\"]\n",
    "conf_interval = 0.95\n",
    "\n",
    "avg_stall_cts = list()\n",
    "avg_stall_cts_conf = list()\n",
    "stdev_stall_cts = list()\n",
    "\n",
    "avg_stall_lengths = list()\n",
    "avg_stall_lengths_conf = list()\n",
    "stdev_stall_lengths = list()\n",
    "\n",
    "stall_start_times = list()\n",
    "\n",
    "for seg_len in seg_lens:\n",
    "    dir_path = \"./{seg_len}\".format(seg_len = seg_len)\n",
    "    \n",
    "    seglen_aggregate_stall_lengths = list()\n",
    "    seglen_aggregate_stall_counts = list()\n",
    "    seglen_aggregate_stall_starts = list()\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        cur_stall_starts_sec, cur_stall_lengths_ms = ComputeStallData(file_path)\n",
    "        seglen_aggregate_stall_lengths += cur_stall_lengths_ms\n",
    "        seglen_aggregate_stall_counts.append(len(cur_stall_lengths_ms))\n",
    "        seglen_aggregate_stall_starts.append(cur_stall_starts_sec)\n",
    "\n",
    "    if len(seglen_aggregate_stall_lengths) == 0:\n",
    "        seglen_aggregate_stall_lengths.append(0)\n",
    "    if len(seglen_aggregate_stall_counts) == 0:\n",
    "        seglen_aggregate_stall_counts.append(0)\n",
    "    \n",
    "    avg_cts = np.mean(seglen_aggregate_stall_counts)\n",
    "    avg_stall_cts.append(avg_cts)\n",
    "    # For some reason, when computing confidence intervals on a list of zeros, it just returns [nan, nan], which seems wrong to me. Manually returning [0, 0].\n",
    "    if all([ct == 0 for ct in seglen_aggregate_stall_counts]):\n",
    "        avg_stall_cts_conf.append([0, 0])\n",
    "    else:\n",
    "        avg_stall_cts_conf.append(st.norm.interval(alpha = conf_interval, loc = avg_cts, scale = st.sem(seglen_aggregate_stall_counts)))\n",
    "    stdev_stall_cts.append(np.std(seglen_aggregate_stall_counts))\n",
    "    \n",
    "    avg_lengths = np.mean(seglen_aggregate_stall_lengths)\n",
    "    avg_stall_lengths.append(avg_lengths)\n",
    "    # For some reason, when computing confidence intervals on a list of zeros, it just returns [nan, nan], which seems wrong to me. Manually returning [0, 0].\n",
    "    if all([ct == 0 for ct in seglen_aggregate_stall_lengths]):\n",
    "        avg_stall_lengths_conf.append([0, 0])\n",
    "    else:\n",
    "        avg_stall_lengths_conf.append(st.norm.interval(alpha = conf_interval, loc = avg_lengths, scale = st.sem(seglen_aggregate_stall_lengths)))\n",
    "    stdev_stall_lengths.append(np.std(seglen_aggregate_stall_lengths))\n",
    "    \n",
    "    stall_start_times.append(seglen_aggregate_stall_starts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf6e73-4cd3-42fe-b9c7-c2a6ec07c2e2",
   "metadata": {},
   "source": [
    "### Stall lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c35a0a-8a18-473f-999f-d9aaee2fff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stall_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549991ea-27e6-466f-808b-ebd16aec3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stall_lengths_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035aeaf-e9c6-4e5d-8ea8-1a17cc90c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_stall_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f72c9e-e30b-42bf-a039-a3e370b54d79",
   "metadata": {},
   "source": [
    "### Stall counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518ad09-f58f-4764-a228-64ecdbaf3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stall_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4cf23-23b9-41ef-8527-63137679bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_stall_cts_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eeb13d-78be-4487-b935-520d985d3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_stall_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e4bb0-6f60-4340-b96a-1ddf8d563e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stalls_permin = np.divide(avg_stall_cts, video_length_min)\n",
    "print(\"Average stalls per minute of video (5s, 10s, 20s): {stallavg}\".format(stallavg = stalls_permin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f05aa-8992-4be2-b636-6d4f9a822f61",
   "metadata": {},
   "source": [
    "### Stall totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6cf71-95ea-456c-9f7b-3ad4ef5345c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stall_totals = np.multiply(avg_stall_cts, avg_stall_lengths)\n",
    "print(\"Stall totals (# of stalls * length of stalls) (sec):\\n{vols}\".format(vols = stall_totals / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6008d2-aea3-4927-aec5-cb71591970b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stall_totals_permin = np.divide(stall_totals, video_length_min)\n",
    "print(\"Average stall totals per minute of video (seconds of stall / minute of video):\\n{avgvol}\".format(avgvol = stall_totals_permin / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f3661-a181-4656-80dd-1196a7d260e3",
   "metadata": {},
   "source": [
    "## Quality Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139747c3-a2b0-4a73-ba83-1c3cdc0dabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_res_pattern = re.compile(\"^LOG  [^,]+,[^,]+,([\\dx]+).+$\")\n",
    "qual_change_pattern = re.compile(\"^QUAL  at [\\d\\.]+ to ([\\dx]+)$\")\n",
    "\n",
    "def ComputeQualChangeData(dir_path):\n",
    "    jsLog_names = [filename for filename in os.listdir(dir_path) if filename.startswith(\"Tester_\") and filename.endswith(\".log\")]\n",
    "    if len(jsLog_names) != 1:\n",
    "        return\n",
    "    jsLog_name = jsLog_names[0]\n",
    "    jsLog = open(os.path.join(dir_path, jsLog_name), \"r\")\n",
    "    \n",
    "    startRes = None\n",
    "    resAmnts = dict()\n",
    "    \n",
    "    # Read through all the lines.\n",
    "    while line := jsLog.readline():\n",
    "        log_res_match = log_res_pattern.match(line)\n",
    "        if log_res_match and startRes == None:\n",
    "            # This is the first LOG line. Let's get the quality level in case it never changes so there are no QUAL entries.\n",
    "            startRes = log_res_match.group(1)\n",
    "            resAmnts[startRes] = resAmnts.get(startRes, 0) + 1\n",
    "        \n",
    "        match = qual_change_pattern.match(line)\n",
    "        if match:\n",
    "            resolution = match.group(1)\n",
    "            resAmnts[resolution] = resAmnts.get(resolution, 0) + 1 # Increase number of occurrences of this resolution by 1.\n",
    "    \n",
    "    return resAmnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558254ba-fcd8-4b7f-b570-eb35942484dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lens = [\"5s\", \"10s\", \"20s\"]\n",
    "avg_qual_changes = list()\n",
    "stdev_qual_changes = list()\n",
    "for seg_len in seg_lens:\n",
    "    #print(\"Segment length: \" + seg_len)\n",
    "    dir_path = \"./{seg_len}\".format(seg_len = seg_len)\n",
    "    \n",
    "    #run_ct = 0\n",
    "    cur_qual_changes = list()\n",
    "    #sum_qual_changes = 0\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "    #for run_idx in range(0, 3): # Index from [0..2].\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        resAmnts = ComputeQualChangeData(file_path)\n",
    "        \n",
    "        #print(resAmnts)\n",
    "        #run_ct += 1\n",
    "        # Subtract one because the video always starts at a certain quality, that's not a quality change.\n",
    "        qual_changes = sum(resAmnts.values()) - 1\n",
    "        cur_qual_changes.append(qual_changes)\n",
    "        #sum_qual_changes += qual_changes\n",
    "        #seg_stall_ct_sum += stall_ct\n",
    "        #seg_stall_len_sum += avg_stall_len_ms\n",
    "        #print(\"Stall count: \" + str(stall_ct))\n",
    "        #print(\"Average stall length (ms): \" + str(avg_stall_len_ms))\n",
    "    \n",
    "    avg_qual_changes.append(np.mean(cur_qual_changes))\n",
    "    stdev_qual_changes.append(np.std(cur_qual_changes))\n",
    "    #avg_qual_changes.append(sum_qual_changes / run_ct)\n",
    "    #avg_stall_cts.append(seg_stall_ct_sum / run_ct)\n",
    "    #avg_stall_lengths.append(seg_stall_len_sum / run_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc938ad-d325-4cf3-966e-4fbf686a7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_qual_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834bd65-7d94-4d11-97d4-beb12c08e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_qual_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f5e76-a043-4169-bca6-7a5d5a764a0d",
   "metadata": {},
   "source": [
    "## Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065fc0a-a97d-46a4-964a-829099d64bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reso_to_bitrate_kbps = {\n",
    "    \"480x270\": 2000,\n",
    "    \"640x360\": 3000,\n",
    "    \"960x540\": 5000,\n",
    "    \"1280x720\": 10000,\n",
    "    \"1920x1080\": 17200,\n",
    "    \"3840x2160\": 40000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5c716-2a6f-47f4-9f69-2516b3dc69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_res_pattern = re.compile(\"^LOG  [^,]+,[^,]+,([\\dx]+).+$\")\n",
    "\n",
    "def ComputeQualData(dir_path):\n",
    "    jsLog_names = [filename for filename in os.listdir(dir_path) if filename.startswith(\"Tester_\") and filename.endswith(\".log\")]\n",
    "    #print(jsLog_names)\n",
    "    if len(jsLog_names) != 1:\n",
    "        return\n",
    "    jsLog_name = jsLog_names[0]\n",
    "    jsLog = open(os.path.join(dir_path, jsLog_name), \"r\")\n",
    "    \n",
    "    resAmnts = dict()\n",
    "    resOccurrences = list()\n",
    "    \n",
    "    # Read through all the lines.\n",
    "    while line := jsLog.readline():\n",
    "        log_res_match = log_res_pattern.match(line)\n",
    "        if not log_res_match:\n",
    "            continue\n",
    "\n",
    "        startRes = log_res_match.group(1)\n",
    "        #print(startRes)\n",
    "        resAmnts[startRes] = resAmnts.get(startRes, 0) + 1\n",
    "        resOccurrences.append(startRes)\n",
    "        \n",
    "    return resOccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a89f9f-1fa9-462a-bffb-b6a1280f1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lens = [\"5s\", \"10s\", \"20s\"]\n",
    "avg_stall_cts = list()\n",
    "avg_stall_lengths = list()\n",
    "resOccurrences = list()\n",
    "for seg_len in seg_lens:\n",
    "    print(\"Segment length: \" + seg_len)\n",
    "    dir_path = \"./{seg_len}\".format(seg_len = seg_len)\n",
    "    \n",
    "    run_ct = 0\n",
    "    seg_stall_ct_sum = 0\n",
    "    seg_stall_len_sum = 0\n",
    "    resQtys = list()\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "    #for run_idx in range(0, 3): # Index from [0..2].\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        #resAmnts = ComputeQualData(file_path)\n",
    "        resQtys += ComputeQualData(file_path)\n",
    "        #print(resAmnts)\n",
    "        #run_ct += 1\n",
    "        #seg_stall_ct_sum += stall_ct\n",
    "        #seg_stall_len_sum += avg_stall_len_ms\n",
    "        #print(\"Stall count: \" + str(stall_ct))\n",
    "        #print(\"Average stall length (ms): \" + str(avg_stall_len_ms))\n",
    "    \n",
    "    resOccurrences.append(resQtys)\n",
    "    #avg_stall_cts.append(seg_stall_ct_sum / run_ct)\n",
    "    #avg_stall_lengths.append(seg_stall_len_sum / run_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90ca04-ebf2-4190-8738-157f9d72389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resOccurrences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840339b7-5999-4c20-a821-477945ca5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "resAmnts = list()\n",
    "#resAmnts = pd.DataFrame()\n",
    "for i in range(0, len(resOccurrences)):\n",
    "    occDict = dict(Counter(resOccurrences[i]))\n",
    "    occurrenceFrame = pd.DataFrame()\n",
    "    occurrenceFrame[\"Resolution\"] = occDict.keys()\n",
    "    occurrenceFrame[\"Occurrences\"] = occDict.values()\n",
    "    totalOccurrences = occurrenceFrame[\"Occurrences\"].sum()\n",
    "    occurrenceFrame[\"Proportion\"] = occurrenceFrame[\"Occurrences\"] / totalOccurrences\n",
    "    resAmnts.append(occurrenceFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe3989-c85a-4948-9cc5-293b86561106",
   "metadata": {},
   "outputs": [],
   "source": [
    "resAmnts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200270a-389b-4b5f-958a-e52781d47cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resAmnts[0].loc[resAmnts[0][\"Occurrences\"].idxmax()]\n",
    "#resAmnts[0].loc[Occurrences == resAmnts[0][\"Occurrences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c748ac-a91d-48f0-9d62-16ad640e962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resAmnts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f826815-5355-4885-b5f9-4f2fe894ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resAmnts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53cfe8c-11d6-4055-8187-0b4c6df63b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "resOccurrencesSorted = [sorted([reso_to_bitrate_kbps[res] for res in runData]) for runData in resOccurrences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f431c-b967-4831-98a1-5456b528a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "resOccurrencesSortedDf = pd.DataFrame()\n",
    "for idx, seglen in enumerate(seg_lens):\n",
    "    #resOccurrencesSortedDf[seglen] = resOccurrencesSorted[idx]\n",
    "    resOccurrencesSortedDf = pd.concat([resOccurrencesSortedDf, pd.DataFrame(resOccurrencesSorted[idx])], axis = 1)\n",
    "resOccurrencesSortedDf.columns = seg_lens\n",
    "#fig = px.ecdf(resOccurrencesSorted[0], ecdfnorm = \"percent\")\n",
    "fig = px.ecdf(resOccurrencesSortedDf.loc[:, \"5s\"], ecdfnorm = \"percent\")\n",
    "#fig.data\n",
    "#fig.data[0].line.color = \"red\"\n",
    "#fig.data[1].line.color = \"green\"\n",
    "#fig.data[2].line.color = \"blue\"\n",
    "#fig.update_xaxes(categoryorder = \"array\", categoryarray = [\"480x270\", \"640x360\", \"960x540\", \"1280x720\", \"1920x1080\", \"3840x2160\"])\n",
    "fig.update_xaxes(title = \"Bitrate\", range = [0, 42000])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85071c-61d7-4987-8f14-09ad2daf2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "resOccurrencesSortedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513cb0d-f6a1-4ec9-b9d9-9c9ab643d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as plgo\n",
    "import plotly.express as px\n",
    "\n",
    "#fig = px.ecdf(resOccurrencesSortedDf, x = seg_lens, ecdfnorm = \"percent\")\n",
    "#fig.update_xaxes(range = [0, 42000])\n",
    "combinedFig = plgo.Figure()\n",
    "seg_len_traces = []\n",
    "for idx, seg_len in enumerate(seg_lens):\n",
    "    seg_len_trace = px.ecdf(resOccurrencesSortedDf.iloc[:, idx], ecdfnorm = \"percent\")\n",
    "    seg_len_traces += seg_len_trace\n",
    "for seg_len_trace in seg_len_traces:\n",
    "    combinedFig.add_trace(seg_len_trace)\n",
    "combinedFig.update_xaxes(range = [0, 42000])\n",
    "combinedFig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c691a53-65ee-4a7f-bbcc-29e903504f44",
   "metadata": {},
   "source": [
    "## RTT Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56d274-7836-42dc-b7c3-b5090513e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def find_missing_and_out_of_order_values(data):\n",
    "    missing_values = []\n",
    "    out_of_order_values = []\n",
    "\n",
    "    for i, value in enumerate(data):\n",
    "        # Check if the current value is missing\n",
    "        if i not in data:\n",
    "            missing_values.append(i)\n",
    "\n",
    "        # Check if the current value is out of order\n",
    "        if i > 0 and value < data[i-1]:\n",
    "            out_of_order_values.append(value)\n",
    "\n",
    "    return missing_values, out_of_order_values\n",
    "\n",
    "def count_elements_layered_list(nested_list):\n",
    "    count = 0\n",
    "    for element in nested_list:\n",
    "        if isinstance(element, list):\n",
    "            count += count_elements_layered_list(element)\n",
    "        else:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def calculate_rtt(folder_path):\n",
    "    all_packet_loss_seqs = []\n",
    "    all_missing_seqs = []\n",
    "    all_out_of_order_seqs = []\n",
    "    rtt_list = []\n",
    "    receive_rates = []\n",
    "\n",
    "    for run_folder in os.listdir(folder_path):\n",
    "        run_folder_path = os.path.join(folder_path, run_folder)\n",
    "        if not os.path.isdir(run_folder_path):\n",
    "            continue\n",
    "        file_path = os.path.join(run_folder_path, \"UDPing_log.csv\")\n",
    "        with open(file_path, \"r\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            next(csv_reader) # skip \"Pinging...\" row\n",
    "            next(csv_reader) # skip \"empty\" row\n",
    "            next(csv_reader) # skip header row\n",
    "            for row in csv_reader:\n",
    "                if \"Ping statistics\" in row[0]:\n",
    "                    break\n",
    "                rtt = float(row[2])\n",
    "                rtt_list.append(rtt)\n",
    "\n",
    "            send=int(csv_reader.__next__()[1])\n",
    "            receive=int(csv_reader.__next__()[1])\n",
    "            if receive == 0:\n",
    "                receive_rates.append(0) # There are cases that all packets got lost.\n",
    "            else:\n",
    "                receive_rates.append(send/receive)\n",
    "\n",
    "        #get the seq that is not one above the previous seq\n",
    "        with open(file_path, \"r\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            prev_seq=-9999\n",
    "            packet_loss_seqs=[]\n",
    "            next(csv_reader)  # skip \"Pinging...\" row\n",
    "            next(csv_reader)  # skip \"empty\" row\n",
    "            next(csv_reader)  # skip header row\n",
    "            for row in csv_reader:\n",
    "                if \"Ping statistics\" in row[0]:\n",
    "                    break\n",
    "\n",
    "                seq = int(row[3])\n",
    "                if prev_seq != -9999 and seq != prev_seq + 1:\n",
    "                    packet_loss_seqs.append(seq)\n",
    "                prev_seq = seq\n",
    "\n",
    "            all_packet_loss_seqs.append(packet_loss_seqs)\n",
    "\n",
    "        #Find missing and out of order values\n",
    "        with open(file_path, \"r\") as csv_file:\n",
    "            seq_column = []\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            next(csv_reader)  # skip \"Pinging...\" row\n",
    "            next(csv_reader)  # skip \"empty\" row\n",
    "            next(csv_reader)  # skip header row\n",
    "            for row in csv_reader:\n",
    "                if \"Ping statistics\" in row[0]:\n",
    "                    break\n",
    "                seq_column.append(int(row[3]))\n",
    "            missing_values, out_of_order_values=find_missing_and_out_of_order_values(seq_column)\n",
    "            all_missing_seqs.append(missing_values)\n",
    "            all_out_of_order_seqs.append(out_of_order_values)\n",
    "\n",
    "    mean_rtt = np.mean(rtt_list)\n",
    "    std_rtt = np.std(rtt_list)\n",
    "    mean_receive_rate=np.mean(receive_rates)\n",
    "    std_receive_rate=np.std(receive_rates)\n",
    "\n",
    "    return (mean_rtt, std_rtt,mean_receive_rate,std_receive_rate, all_packet_loss_seqs, all_missing_seqs, all_out_of_order_seqs)\n",
    "\n",
    "mean_rtt, std_rtt, mean_receive_rate, std_receive_rate, packet_loss_seqs, missing_seqs, out_of_order_seqs= calculate_rtt(\".\\\\5s\")\n",
    "print(\"Mean RTT:\", mean_rtt)\n",
    "print(\"Standard deviation of RTT:\", std_rtt)\n",
    "print(\"Mean Receive rate:\", mean_receive_rate)\n",
    "print(\"Standard deviation of Receive rate:\", std_receive_rate)\n",
    "print(\"Packet loss sequences:\", packet_loss_seqs)\n",
    "print(\"Packet loss sequences count:\", count_elements_layered_list(packet_loss_seqs))\n",
    "print(\"Missing sequences:\",missing_seqs)\n",
    "print(\"Missing sequences count:\", count_elements_layered_list(missing_seqs))\n",
    "print(\"Out of order sequences:\",out_of_order_seqs)\n",
    "print(\"Out of order sequences count:\",count_elements_layered_list(out_of_order_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56a3ea-d53f-489c-8439-6dcb4cd73f1c",
   "metadata": {},
   "source": [
    "## Estimated Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600dade-b69c-4842-a9d2-bff1afe35374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "estimated_throughput_pattern = re.compile(\"^LOG  ([\\d\\.]+),(?:[^,]+,){4}([\\d\\.]+)$\")\n",
    "\n",
    "def ComputeEstTputData(dir_path):\n",
    "    jsLog_names = [filename for filename in os.listdir(dir_path) if filename.startswith(\"Tester_\") and filename.endswith(\".log\")]\n",
    "    if len(jsLog_names) != 1:\n",
    "        return\n",
    "    jsLog_name = jsLog_names[0]\n",
    "    jsLog = open(os.path.join(dir_path, jsLog_name), \"r\")\n",
    "    \n",
    "    times = list()\n",
    "    esttputs = list()\n",
    "    \n",
    "    # Read through all the lines.\n",
    "    while line := jsLog.readline():\n",
    "        esttput_match = estimated_throughput_pattern.match(line)\n",
    "        if not esttput_match:\n",
    "            continue\n",
    "\n",
    "        time = float(esttput_match.group(1))\n",
    "        times.append(time)\n",
    "        esttput = float(esttput_match.group(2))\n",
    "        esttputs.append(esttput)\n",
    "        \n",
    "    return times, esttputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c5fdc-8c1e-4b94-b2a0-7e16236ea88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "settings = [\"5s\", \"10s\", \"20s\"]\n",
    "estimated_times = list()\n",
    "estimated_throughputs = list()\n",
    "for setting in settings:\n",
    "    dir_path = \"./{setting}\".format(setting = setting)\n",
    "    \n",
    "    setting_times = list()\n",
    "    setting_esttputs = list()\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        \n",
    "        run_times, run_esttputs = ComputeEstTputData(file_path)\n",
    "        setting_times.append(run_times)\n",
    "        setting_esttputs.append(run_esttputs)\n",
    "    \n",
    "    estimated_times.append(setting_times)\n",
    "    estimated_throughputs.append(setting_esttputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cf146-fad6-4535-83ba-289c4729fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as plx\n",
    "import plotly.graph_objects as plgo\n",
    "\n",
    "for setting_idx in range(0, len(estimated_throughputs)):\n",
    "    setting_tputs = list()\n",
    "    \n",
    "    fig = plgo.Figure()\n",
    "    set_std_plot_params(fig)\n",
    "    \n",
    "    for run_idx in range(len(estimated_throughputs[setting_idx])):\n",
    "        run_times = estimated_times[setting_idx][run_idx]\n",
    "        run_tputs = estimated_throughputs[setting_idx][run_idx]\n",
    "        setting_tputs += run_tputs\n",
    "        #fig.add_scatter(y = estimated_throughputs[setting_idx][run_idx], name = \"Run {idx}\".format(idx = run_idx))\n",
    "        fig.add_scatter(x = run_times, y = run_tputs,\n",
    "                        name = \"Run {idx}\".format(idx = run_idx))\n",
    "    print(\"Mean:\", np.mean(setting_tputs))\n",
    "    print(\"Stdev:\", np.std(setting_tputs))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3c9cc-8318-43bf-bcd6-3c5cd4986cf1",
   "metadata": {},
   "source": [
    "## Measured Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f819750-8ec6-4758-a3b5-d30e98c3d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def ComputeMeasuredTputData(dir_path):\n",
    "    packetLog_names = [filename for filename in os.listdir(dir_path) if filename == \"packets_client.csv\"]\n",
    "    if len(packetLog_names) != 1:\n",
    "        return\n",
    "    packetLog_name = packetLog_names[0]\n",
    "    packetLog = open(os.path.join(dir_path, packetLog_name), \"r\")\n",
    "    \n",
    "    measuredTputs = list()\n",
    "    \n",
    "    packetDf = pd.read_csv(packetLog, usecols=[0, 1])\n",
    "    \n",
    "    def grouping_attr(index):\n",
    "        return math.floor(packetDf['_ws.col.Time'].loc[index])\n",
    "\n",
    "    packetDf = packetDf.groupby(grouping_attr)['frame.len'].sum().reset_index()\n",
    "    #packetDf['frame.len'] = packetDf['frame.len'].multiply(8e-6)  # multiply to get Mbit/s\n",
    "    packetDf['frame.len'] = packetDf['frame.len'].multiply(8e-3)  # multiply to get Kbit/s\n",
    "        \n",
    "    return list(packetDf[\"index\"]), list(packetDf[\"frame.len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345fade-32b3-402e-ab3e-d4eff5e754fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "settings = [\"5s\", \"10s\", \"20s\"]\n",
    "measured_times = list()\n",
    "measured_throughputs = list()\n",
    "for setting in settings:\n",
    "    dir_path = \"./{setting}\".format(setting = setting)\n",
    "    \n",
    "    setting_times = list()\n",
    "    setting_measuredTputs = list()\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        \n",
    "        run_times, run_measuredTputs = ComputeMeasuredTputData(file_path)\n",
    "        setting_times.append(run_times)\n",
    "        setting_measuredTputs.append(run_measuredTputs)\n",
    "    \n",
    "    measured_times.append(setting_times)\n",
    "    measured_throughputs.append(setting_measuredTputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cf45b-4b3c-472f-9e64-d789dab6b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as plx\n",
    "import plotly.graph_objects as plgo\n",
    "\n",
    "for setting_idx in range(len(measured_throughputs)):\n",
    "    setting_tputs = list()\n",
    "    \n",
    "    fig = plgo.Figure()\n",
    "    set_std_plot_params(fig)\n",
    "    \n",
    "    for run_idx in range(len(measured_throughputs[setting_idx])):\n",
    "        run_tputs = measured_throughputs[setting_idx][run_idx]\n",
    "        setting_tputs += run_tputs\n",
    "        fig.add_scatter(y = measured_throughputs[setting_idx][run_idx], name = \"Run {idx}\".format(idx = run_idx))\n",
    "        \n",
    "    print(\"Mean:\", np.mean(setting_tputs))\n",
    "    print(\"Stdev:\", np.std(setting_tputs))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbecf20-b71c-45e2-85cc-25f9233ba9c8",
   "metadata": {},
   "source": [
    "## Estimated vs Measured Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3418e-dce1-46cb-8cf8-21486459940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_idx = 0\n",
    "for run_idx in range(len(estimated_throughputs[setting_idx])):\n",
    "    run_estimated_times = estimated_times[setting_idx][run_idx]\n",
    "    run_estimated_tputs = estimated_throughputs[setting_idx][run_idx]\n",
    "    \n",
    "    run_measured_times = measured_times[setting_idx][run_idx]\n",
    "    run_measured_tputs = measured_throughputs[setting_idx][run_idx]\n",
    "    \n",
    "    fig = plgo.Figure()\n",
    "    \n",
    "    set_std_plot_params(fig)\n",
    "    \n",
    "    fig.add_scatter(x = run_estimated_times, y = run_estimated_tputs,\n",
    "                    name = \"Estimated\".format(idx = run_idx))\n",
    "    fig.add_scatter(x = run_measured_times, y = run_measured_tputs,\n",
    "                    name = \"Measured\".format(idx = run_idx))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5523c7-11ea-4d86-94af-2f13ef0d9af6",
   "metadata": {},
   "source": [
    "## Time to Start Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f9176-a19c-40f3-ae98-128119edc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "log_video_time_pattern = re.compile(\"^LOG  ([\\d\\.]+),([\\d\\.]+):.+$\")\n",
    "#stall_pattern = re.compile(\"^STALL  ([\\d]+) ms and stopped at [\\d\\.]+ sec\", re.M)\n",
    "\n",
    "def ComputeTimeToStart(dir_path):\n",
    "    jsLog_names = [filename for filename in os.listdir(dir_path) if filename.startswith(\"Tester_\") and filename.endswith(\".log\")]\n",
    "    if len(jsLog_names) != 1:\n",
    "        return\n",
    "    jsLog_name = jsLog_names[0]\n",
    "    jsLog = open(os.path.join(dir_path, jsLog_name), \"r\")\n",
    "    \n",
    "    # Read through all the lines.\n",
    "    while line := jsLog.readline():\n",
    "        match = log_video_time_pattern.match(line)\n",
    "        if match:\n",
    "            absolute_time = float(match.group(1))\n",
    "            video_time = float(match.group(2))\n",
    "            if video_time > 0:\n",
    "                # We've found when the video started playing.\n",
    "                return absolute_time\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1526a-b1c5-40df-8534-6250fbe7b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import os\n",
    "\n",
    "conf_interval = 0.95\n",
    "seg_lens = [\"5s\", \"10s\", \"20s\"]\n",
    "\n",
    "avg_start_times_sec = list()\n",
    "avg_start_times_sec_conf = list()\n",
    "stdev_start_times_sec = list()\n",
    "\n",
    "for seg_len in seg_lens:\n",
    "    dir_path = \"./{seg_len}\".format(seg_len = seg_len)\n",
    "    \n",
    "    start_times_sec = list()\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        if not os.path.isdir(file_path): # Only looking to read directories (that are symlinks).\n",
    "            continue\n",
    "        start_time_sec = ComputeTimeToStart(file_path)\n",
    "        start_times_sec.append(start_time_sec)\n",
    "    \n",
    "    avg_start_time_sec = np.mean(start_times_sec)\n",
    "    avg_start_times_sec.append(avg_start_time_sec)\n",
    "    #avg_start_times_sec_conf.append(st.norm.interval(alpha = conf_interval, loc = avg_start_time_sec, scale = st.sem(start_times_sec)))\n",
    "    avg_start_times_sec_conf.append(st.t.interval(alpha = conf_interval,\n",
    "                                                  df = len(start_times_sec) - 1,\n",
    "                                                  loc = avg_start_time_sec,\n",
    "                                                  scale = st.sem(start_times_sec)))\n",
    "    stdev_start_times_sec.append(np.std(start_times_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd64b7a-53ac-4967-8a3a-20526e69897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_start_times_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01184196-31bb-4b55-9690-5204dde2a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_start_times_sec_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3938d92f-cd6a-427f-bce6-1abbf86644ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_start_times_sec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
